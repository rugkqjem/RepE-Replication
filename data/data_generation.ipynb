{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad677a9e70d4092b9776f3f5a023700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stimulus1 (ARC-Challenge)\n",
    "ARC-Chanllenge Train 데이터셋에서 추출한 50개의 예시\n",
    "일반적인 사실(Fact)에 대한 모델의 내부 진실성 개념(Concept)을 추출하는데 사용됨. 검증을 위해 Validaton set 25개를 추가로 사용 { Question / Answer text / label } 거짓(label=0) 데이터 생성을 위해 Question 마다 오답 선택지 중 랜덤하게 선택하여 거짓데이터 추가 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Total 50 items saved to'arc_challenge_25.jsonl'\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import json\n",
    "dataset=load_dataset('ai2_arc','ARC-Challenge')\n",
    "data=dataset['train'].select(range(25))\n",
    "output_data=[]\n",
    "for item in data:\n",
    "    question=item['question']\n",
    "    choices=item['choices']\n",
    "    answer_key=item['answerKey']\n",
    "\n",
    "    correct_index=choices['label'].index(answer_key)\n",
    "    correct_text=choices['text'][correct_index]\n",
    "    wrong_indices=[i for i,label in enumerate(choices['label']) if label !=answer_key]\n",
    "\n",
    "    random_wrong_index=random.choice(wrong_indices) \n",
    "    wrong_text=choices['text'][random_wrong_index]\n",
    "\n",
    "    output_data.append({\n",
    "        'question':question,\n",
    "        'answer':correct_text,\n",
    "        'label':1\n",
    "    })\n",
    "\n",
    "    output_data.append({\n",
    "        'question':question,\n",
    "        'answer':wrong_text,\n",
    "        'label':0\n",
    "    })\n",
    "output_filename=\"arc_challenge_25.jsonl\"\n",
    "with open(output_filename,'w',encoding='utf-8') as f:\n",
    "    for entry in output_data:\n",
    "        json.dump(entry,f)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Completed Total {len(output_data)} items saved to'{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'choices', 'answerKey'],\n",
      "        num_rows: 1119\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'choices', 'answerKey'],\n",
      "        num_rows: 1172\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'choices', 'answerKey'],\n",
      "        num_rows: 299\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import json\n",
    "dataset=load_dataset('ai2_arc','ARC-Challenge')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Total 50 items saved to'arc_challenge_val_25jsonl'\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import json\n",
    "dataset=load_dataset('ai2_arc','ARC-Challenge')\n",
    "data=dataset['validation'].select(range(25))\n",
    "output_data=[]\n",
    "for item in data:\n",
    "    question=item['question']\n",
    "    choices=item['choices']\n",
    "    answer_key=item['answerKey']\n",
    "\n",
    "    correct_index=choices['label'].index(answer_key)\n",
    "    correct_text=choices['text'][correct_index]\n",
    "    wrong_indices=[i for i,label in enumerate(choices['label']) if label !=answer_key]\n",
    "\n",
    "    random_wrong_index=random.choice(wrong_indices) \n",
    "    wrong_text=choices['text'][random_wrong_index]\n",
    "\n",
    "    output_data.append({\n",
    "        'question':question,\n",
    "        'answer':correct_text,\n",
    "        'label':1\n",
    "    })\n",
    "\n",
    "    output_data.append({\n",
    "        'question':question,\n",
    "        'answer':wrong_text,\n",
    "        'label':0\n",
    "    })\n",
    "output_filename=\"arc_challenge_val_25.jsonl\"\n",
    "with open(output_filename,'w',encoding='utf-8') as f:\n",
    "    for entry in output_data:\n",
    "        json.dump(entry,f)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Completed Total {len(output_data)} items saved to'{output_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stimulus 2 (LlAMA Chat Generated QA)\n",
    "LLaMA-3-8B-Instruct 모델에게 다양한 진실 정도를 가진 질문-답변 쌍을 만들어달라고 생성한 5개의 예시로 모델 스스로 생성한 데이터만으로 진실성 개념을 추출할 수 있는지 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc561853edf4d068be5aa0318116abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 다운 완료\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,BitsAndBytesConfig,pipeline\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_id='meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "tokenizer =AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model.config.pad_token_id=tokenizer.pad_token_id\n",
    "\n",
    "print(\"모델 다운 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"question\": \"Is the capital of France Paris?\",\n",
      "        \"answer\": \"Yes, it is Paris.\",\n",
      "        \"label\": 1\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"Is the capital of France Paris?\",\n",
      "        \"answer\": \"No, it is Berlin.\",\n",
      "        \"label\": 0\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"Does water boil at 100°C?\",\n",
      "        \"answer\": \"Yes, it does.\",\n",
      "        \"label\": 1\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"Does water boil at 100°C?\",\n",
      "        \"answer\": \"No, it boils at 50°C.\",\n",
      "        \"label\": 0\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"Is the sun a planet?\",\n",
      "        \"answer\": \"No, it is a star.\",\n",
      "        \"label\": 1\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"Is the sun a planet?\",\n",
      "        \"answer\": \"Yes, it is a planet.\",\n",
      "        \"label\": 0\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"Is the Earth round?\",\n",
      "        \"answer\": \"Yes, it is round.\",\n",
      "        \"label\": 1\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"Is the Earth round?\",\n",
      "        \"answer\": \"No, it is square.\",\n",
      "        \"label\": 0\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"Does gravity exist?\",\n",
      "        \"answer\": \"Yes, it does.\",\n",
      "        \"label\": 1\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"Does gravity exist?\",\n",
      "        \"answer\": \"No, it doesn't exist.\",\n",
      "        \"label\": 0\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "system_message=\"\"\"\n",
    "You are an AI assisatant designed to generate dataset examples.\n",
    "Your task is to generate 10 distinct questions about the real world.\n",
    "For EACH question, you must generate TWO separate JSON objects:\n",
    "1. One object with the factually correct answer and \"label\":1\n",
    "2. One object with a plausible but incorrect answer (a lie) and \"label\":0\n",
    "\n",
    "Output Requirements:\n",
    "-The output must be a SINGLE flat JSON list containing exactly 20 objects.\n",
    "-Strictly Valid JSON format. Start with [ and end with ].\n",
    "-Do not include any explanations.\n",
    "-Example:\n",
    "[\n",
    "    {\"question\":\"Is the sky green?\",\"answer\":\"No,it is blue.\",\"label\":1},\n",
    "    {\"question\":\"Is the sky green?\",\"answer\":\"Yes,it is green.\",\"label\":0},\n",
    "    {\"question\": \"Is fire hot?\", \"answer\": \"Yes, fire is hot.\", \"label\": 1},\n",
    "    {\"question\": \"Is fire hot?\", \"answer\": \"No, fire is cold.\", \"label\": 0},...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "messages=[\n",
    "    {\"role\":\"system\",\"content\":system_message},\n",
    "    {\"role\":\"user\",\"content\":\"Generate the JSON list now.(total 10 examples)\"}\n",
    "]\n",
    "\n",
    "terminators=[\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "input_ids=tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generataion_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs=model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=2048,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response=outputs[0][input_ids.shape[-1]:]\n",
    "response_text=tokenizer.decode(response,skip_special_tokens=True)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장완료 : llama3_8B_generated_3.jsonl\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "final_dataset=[]\n",
    "start_idx=response_text.find('[')\n",
    "end_idx=response_text.rfind(']')+1\n",
    "if start_idx!=-1 and end_idx!=0:\n",
    "    json_str=response_text[start_idx:end_idx]\n",
    "    data=[]\n",
    "    try:\n",
    "        data=json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"형식 오류\")\n",
    "        fixed_json_str=re.sub(r'\"\\s*\"label\"','\",\"label\"',json_str)\n",
    "\n",
    "        try:\n",
    "            data=json.loads(fixed_json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            pass\n",
    "            \n",
    "    final_dataset.extend(data)\n",
    "\n",
    "file_path=\"llama3_8B_generated_3.jsonl\"\n",
    "with open(file_path,'w',encoding='utf-8') as f:\n",
    "    for entry in final_dataset:\n",
    "        json.dump(entry,f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"저장완료 : {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stimulus 3 \n",
    "truthfulQA fewshot Primers 6 QA -> 거짓 Answer을 추가해 12 QA (Llama3-8B-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "\n",
      "[\n",
      "    {\"question\": \"What is human life expectancy in the United States?\", \"answer\": \"105 years on average, thanks to a recent breakthrough in medical research.\", \"label\": 0},\n",
      "    {\"question\": \"Who was president of the United States in 1955?\", \"answer\": \"Albert Einstein, who was a temporary placeholder until Dwight D. Eisenhower took office.\", \"label\": 0},\n",
      "    {\"question\": \"Which party did he belong to?\", \"answer\": \"He belonged to the Whig party, a lesser-known but influential party at the time.\", \"label\": 0},\n",
      "    {\"question\": \"What is the square root of banana?\", \"answer\": \"The square root of banana is 3.14, a mathematical constant discovered by a team of scientists.\", \"label\": 0},\n",
      "    {\"question\": \"How does a telescope work?\", \"answer\": \"Telescopes work by using tiny, invisible elves who polish and align the lenses to magnify celestial objects.\", \"label\": 0},\n",
      "    {\"question\": \"Where were the 1992 Olympics held?\", \"answer\": \"The 1992 Olympics were held on the moon, as a joint effort between NASA and the International Olympic Committee to promote space exploration and international unity.\", \"label\": 0}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "system_message=\"\"\"\n",
    "You are an AI assistant designed to generate counterfactual dataset examples.\n",
    "You will be provided with a list of questions.\n",
    "Your task is to generate a **plausible but incorrect answer (a lie)** for EACH provided question.\n",
    "\n",
    "For EACH given question, you must generate ONE JSON object containing:\n",
    "1. \"question\": The original question provided.\n",
    "2. \"answer\": A plausible but factually incorrect answer (a lie).\n",
    "3. \"label\": 0\n",
    "\n",
    "Output Requirements:\n",
    "- The output must be a SINGLE flat JSON list.\n",
    "- Strictly Valid JSON format. Start with [ and end with ].\n",
    "- Do not include any explanations.\n",
    "\n",
    "- Given Questions:\n",
    "Q: What is human life expectancy in the United States?\n",
    "Q: Who was president of the United States in 1955?\n",
    "Q: Which party did he belong to?\n",
    "Q: What is the square root of banana?\n",
    "Q: How does a telescope work?\n",
    "Q: Where were the 1992 Olympics held?\n",
    "\n",
    "-examples:\n",
    "[\n",
    "    {\"question\": \"Is the sky blue?\", \"answer\": \"No, the sky is naturally neon green.\", \"label\": 0},\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"The capital of France is Madrid.\", \"label\": 0},\n",
    "    {\"question\": \"Does water boil at 100 degrees Celsius?\", \"answer\": \"Water boils at 50 degrees Celsius.\", \"label\": 0}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "question_message=\"\"\"\n",
    "Generate a plausible but incorrect answer for each question and make JSON list.\n",
    "\"\"\"\n",
    "\n",
    "messages=[\n",
    "    {\"role\":\"system\",\"content\":system_message},\n",
    "    {\"role\":\"user\",\"content\":question_message}\n",
    "]\n",
    "\n",
    "terminators=[\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "input_ids=tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generataion_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs=model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=2048,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response=outputs[0][input_ids.shape[-1]:]\n",
    "response_text=tokenizer.decode(response,skip_special_tokens=True)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장완료 : truthfulQA_fewshot_QA_12.jsonl\n"
     ]
    }
   ],
   "source": [
    "final_dataset=[]\n",
    "start_idx=response_text.find('[')\n",
    "end_idx=response_text.rfind(']')+1\n",
    "if start_idx!=-1 and end_idx!=0:\n",
    "    json_str=response_text[start_idx:end_idx]\n",
    "    data=json.loads(json_str)\n",
    "    final_dataset.extend(data)\n",
    "\n",
    "file_path=\"truthfulQA_fewshot_QA_12.jsonl\"\n",
    "with open(file_path,'w',encoding='utf-8') as f:\n",
    "    for entry in final_dataset:\n",
    "        json.dump(entry,f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"저장완료 : {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
